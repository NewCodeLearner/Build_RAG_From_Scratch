{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n6hhDs37vPx"
      },
      "source": [
        "# Create and run a local RAG pipeline from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BD1JsIh7vP3"
      },
      "source": [
        "The goal of this notebook is to build a RAG (Retrieval Augmented Generation) pipeline from scratch and have it run on a local GPU.\n",
        "\n",
        "Specifically, we'd like to be able to open a PDF file, ask questions (queries) of it and have them answered by a Large Language Model (LLM).\n",
        "\n",
        "There are frameworks that replicate this kind of workflow, including LlamaIndex and LangChain, however, the goal of building from scratch is to be able to inspect and customize all the parts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jio4g4__7vP4"
      },
      "source": [
        "# What is RAG?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E_GP3cea7vP4"
      },
      "source": [
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "It was introduced in the paper  [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.](https://arxiv.org/abs/2005.11401)\n",
        "\n",
        "Each step can be roughly broken down to:\n",
        "\n",
        "Retrieval - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.    \n",
        "Augmented - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).    \n",
        "Generation - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimGI7BZ7vP5"
      },
      "source": [
        "# Why RAG?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxJcIz-g7vP5"
      },
      "source": [
        "The main goal of RAG is to improve the generation outputs of LLMs.\n",
        "\n",
        "Two Primary improvements can be seen as:\n",
        "1.Preventing Hallucinations: LLMs are incredible but they are prone to potential hallucinations as in generating something that looks correct but it isn't.RAG pipelines can help LLMs generate more factual outputs by providing them with the factual (retrived) inputs. and even if the generated answer from RAG pipeline doesn't seem correct, because of retrieval, you also have access to the sources where it came from.\n",
        "2.Work with custom data : Many base LLMs are trained with internet scale text data. This means they have great ability to model language, however they often lack specific knowldge. RAG systems can provide LLMs with domain specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.\n",
        "\n",
        "RAG can also be a much quicker solution to implement than fine-tuning an LLM on specific data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP6iZOFK7vP6"
      },
      "source": [
        "# What kind of problems can RAG be used for?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Y7O2DK7vP6"
      },
      "source": [
        "RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n",
        "\n",
        "For example you could use RAG for:\n",
        "\n",
        "Customer support Q&A chat - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\". Klarna, a large financial company, uses a system like this to save $40M per year on customer support costs.\n",
        "Email chain analysis - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n",
        "Company internal documentation chat - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n",
        "Textbook Q&A - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n",
        "All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n",
        "\n",
        "From this angle, you can consider an LLM a calculator for words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huiZvGcH7vP6"
      },
      "source": [
        "# Why local?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rl5h3LGf7vP7"
      },
      "source": [
        "Privacy, speed, cost.\n",
        "\n",
        "Running locally means you use your own hardware.\n",
        "\n",
        "From a privacy standpoint, this means you don't have send potentially sensitive data to an API.\n",
        "\n",
        "From a speed standpoint, it means you won't necessarily have to wait for an API queue or downtime, if your hardware is running, the pipeline can run.\n",
        "\n",
        "And from a cost standpoint, running on your own hardware often has a heavier starting cost but little to no costs after that.\n",
        "\n",
        "Performance wise, LLM APIs may still perform better than an open-source model running locally on general tasks but there are more and more examples appearing of smaller, focused models outperforming larger models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7j7fmV4a7vP7"
      },
      "source": [
        "# What we're going to build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zwL-sEI7vP7"
      },
      "source": [
        "We are going to build RAG pipeline which enables us to chat with a PDF document.\n",
        "\n",
        "We will write the code toL\n",
        "1.Open a PDF document\n",
        "2.Format the text of the PDF ready for an embedding model(this process is known as text splitting/chunking)\n",
        "3.Embed all of the chunks of text in the pdf and turn them into numerical representation which we can store for later.\n",
        "4.Build a retrival system that uses vector search to find relevant chunks of the text based on a query.\n",
        "5.Create a prompt that incorporates the retrieved pieces of text.\n",
        "6.Generate an answer to a query on passages from the textbook.\n",
        "\n",
        "The above steps can be broken down into two major sections:\n",
        "1. Document preprocessing/embedding creation ( Step 1-3)\n",
        "2. Search and answer(steps 4-6)\n",
        "\n",
        "It's similar to the workflow outlined on the NVIDIA blog which details a local RAG pipeline. https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements and setup"
      ],
      "metadata": {
        "id": "Egl8-rT89sja"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "Thlq90W67vP8"
      },
      "outputs": [],
      "source": [
        "# Perform Google Colab installs (if running in Google Colab)\n",
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Document/Text Processing and Embedding Creation\n",
        "\n",
        "Ingredients:\n",
        "\n",
        "1.   PDF document of choice\n",
        "2.   Embedding mode of choice\n",
        "\n",
        "Steps:\n",
        "\n",
        "Import PDF document.\n",
        "\n",
        "1.   Process text for embedding (e.g. split into chunks fo sentences)\n",
        "2.   Embed text chunks with embedding model.\n",
        "3.   Save embeddings to file for later use(embeddings will store on file for many years or until you lose your hard drive)\n"
      ],
      "metadata": {
        "id": "SD_ATOkJ-LOF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Import PDF Document**\n",
        "\n",
        "This will work with many other kinds of documents.\n",
        "\n",
        "However, we'll start with PDF since many people have PDFs.\n",
        "\n",
        "But just keep in mind, text files, email chains, support documentation, articles and more can also work.\n",
        "\n",
        "There are several libraries to open PDFs with Python but I found that PyMuPDF works quite well in many cases.\n",
        "\n",
        "First we'll download the PDF if it doesn't exist."
      ],
      "metadata": {
        "id": "4xcbEWnX-bVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download PDF file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Get PDF document\n",
        "pdf_path = \"human-nutrition-text.pdf\"\n",
        "\n",
        "# Download PDF if it doesn't already exist\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "  # The URL of the PDF you want to download\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "\n",
        "  # The local filename to save the downloaded file\n",
        "  filename = pdf_path\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Open a file in binary write mode and save the content to it\n",
        "      with open(filename, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"The file has been downloaded and saved as {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "  print(f\"File {pdf_path} exists.\")"
      ],
      "metadata": {
        "id": "6xXmZLAiAEFw",
        "outputId": "6696a04d-d106-4749-a0c2-37770a05827d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File human-nutrition-text.pdf exists.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}